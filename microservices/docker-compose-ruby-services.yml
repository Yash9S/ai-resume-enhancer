services:
  # Redis (for job queues)
  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data

  # Ruby AI Service (uses existing external Ollama)
  ruby-ai-service:
    build: ./ruby-ai-service
    ports:
      - "4001:4001"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - RAILS_ENV=development
    volumes:
      - ./ruby-ai-service:/app
    restart: unless-stopped

volumes:
  redis_data: