services:
  # Redis (for job queues)
  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data

  # Ruby AI Service (uses existing external Ollama)
  ruby-ai-service:
    build: ./ruby-ai-service
    ports:
      - "8001:3000"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - RAILS_ENV=development
      - PORT=3000
    volumes:
      - ./ruby-ai-service:/app
    restart: unless-stopped

volumes:
  redis_data: